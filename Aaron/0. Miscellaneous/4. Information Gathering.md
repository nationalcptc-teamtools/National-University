## WHOIS

Whois database is a searchable list of all domains currently registered worldwide.

|                                                                  |                                                                              |
| ---------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| Command                                                          | Description                                                                  |
| export TARGET="domain.tld"                                       | Assign target to an environment variable. "TARGET" is left as word "TARGET". |
| whois $TARGET                                                    | WHOIS lookup for the target.                                                 |
| [https://whois.domaintools.com/](https://whois.domaintools.com/) | Online version of WHOIS                                                      |

## DNS Enumeration

|   |   |
|---|---|
|Command|Description|
|export TARGET="facebook.com"||
|nslookup $TARGET|Identify the A record for the target domain.|
|nslookup -query=A $TARGET|Querying: A Records for a Subdomain.|
|dig $TARGET @<nameserver/IP>|Identify the A record for the target domain.|
|dig a $TARGET @<nameserver/IP><br><br>Example:<br><br>dig a [www.facebook.com](http://www.facebook.com) @1.1.1.1|Querying: A Records for a Subdomain.|
|nslookup -query=PTR <IP><br><br>Example:<br><br>nslookup -query=PTR 31.13.92.36|Identify the PTR record for the target IP address.|
|dig -x <IP> @<nameserver/IP><br><br>Example:<br><br>dig -x 31.13.92.36 @1.1.1.1|Identify the PTR record for the target IP address.|
|nslookup -query=ANY $TARGET|Identify ANY records for the target domain.|
|dig any $TARGET @<nameserver/IP><br><br>Example:<br><br>dig any google.com @8.8.8.8|Identify ANY records for the target domain.|
|nslookup -query=TXT $TARGET|Identify the TXT records for the target domain.|
|dig txt $TARGET @<nameserver/IP><br><br>Example:<br><br>dig txt facebook.com @1.1.1.1|Identify the TXT records for the target domain.|
|nslookup -query=MX $TARGET|Identify the MX records for the target domain.|
|dig mx $TARGET @<nameserver/IP><br><br>Example:<br><br>dig mx facebook.com @1.1.1.1|Identify the MX records for the target domain.|

  
 

## Passive Subdomain Enumeration

|   |   |
|---|---|
|Resource/Command|Description|
|VirusTotal|[https://www.virustotal.com/gui/home/url](https://www.virustotal.com/gui/home/url)|
|Censys (examine certificate logs for domain enumeration)|[https://censys.io/](https://censys.io/)|
|Crt.sh (examine certificate logs for domain enumeration)|[https://crt.sh/](https://crt.sh/)|
|export TARGET="facebook.com"<br><br>curl -s "[https://crt.sh/?q=${TARGET}&output=json](https://crt.sh/?q=$%7bTARGET%7d&output=json)" \| jq -r '.[] \| "\(.name_value)\n\(.common_name)"' \| sort -u > "${TARGET}_crt.sh.txt"|Issue the request with minimal output, Ask for the json output, process the json output and print certificate's name value and common name one per line, sort alphabetically the output provided and removes duplicates.|
|curl -s [https://sonar.omnisint.io/subdomains/{domain](https://sonar.omnisint.io/subdomains/%7bdomain)} \| jq -r '.[]' \| sort -u|All subdomains for a given domain.|
|curl -s [https://sonar.omnisint.io/tlds/{domain](https://sonar.omnisint.io/tlds/%7bdomain)} \| jq -r '.[]' \| sort -u|All TLDs found for a given domain.|
|curl -s [https://sonar.omnisint.io/all/{domain](https://sonar.omnisint.io/all/%7bdomain)} \| jq -r '.[]' \| sort -u|All results across all TLDs for a given domain.|
|curl -s [https://sonar.omnisint.io/reverse/{ip](https://sonar.omnisint.io/reverse/%7bip)} \| jq -r '.[]' \| sort -u|Reverse DNS lookup on IP address.|
|curl -s [https://sonar.omnisint.io/reverse/{ip}/{mask](https://sonar.omnisint.io/reverse/%7bip%7d/%7bmask)} \| jq -r '.[]' \| sort -u|Reverse DNS lookup of a CIDR range.|
|curl -s "[https://crt.sh/?q=${TARGET}&output=json](https://crt.sh/?q=$%7bTARGET%7d&output=json)" \| jq -r '.[] \| "\(.name_value)\n\(.common_name)"' \| sort -u|Certificate Transparency.|
|create a file called sources.txt with the following contents:<br><br>baidu<br><br>bufferoverun<br><br>crtsh<br><br>hackertarget<br><br>otx<br><br>projecdiscovery<br><br>rapiddns<br><br>sublist3r<br><br>threatcrowd<br><br>trello<br><br>urlscan<br><br>vhost<br><br>virustotal<br><br>zoomeye<br><br>Then<br><br>export TARGET="facebook.com"<br><br>Then<br><br>cat sources.txt \| while read source; do theHarvester -d "${TARGET}" -b $source -f "${source}-${TARGET}";done<br><br>When the process finishes, we can extract all the subdomains found and sort them via the following command:<br><br>cat *.json \| jq -r '.hosts[]' 2>/dev/null \| cut -d':' -f 1 \| sort -u > "${TARGET}_theHarvester.txt"<br><br>Now we can merge all the passive reconnaissance files via:<br><br>cat facebook.com_*.txt \| sort -u > facebook.com_subdomains_passive.txt<br><br>cat facebook.com_subdomains_passive.txt \| wc -l|Using TheHarvester<br><br>Searching for subdomains and other information on the sources provided in the source.txt list.|

  
 

## Passive Infrastructure Identification

|   |   |
|---|---|
|Resource/Command|Description|
|Netcraft - offer us information about the servers without even interacting with them|[https://www.netcraft.com/](https://www.netcraft.com/)|
|WayBackMachine|[http://web.archive.org/](http://web.archive.org/)|
|WayBackURLs - to inspect URLs saved by Wayback Machine and look for specific keywords.|[https://github.com/tomnomnom/waybackurls](https://github.com/tomnomnom/waybackurls)|
|anasec@htb[/htb]$ go install github.com/tomnomnom/waybackurls@latest<br><br>waybackurls -dates [https://$TARGET](https://$TARGET) > waybackurls.txt<br><br>cat waybackurls.txt|Crawling URLs from a domain with the date it was obtained.|

  
 

## Active Infrastructure Identification

|   |   |
|---|---|
|Resource/Command|Description|
|curl -I "[http://${TARGET](http://$%7bTARGET)}"|Display HTTP headers of the target webserver.|
|whatweb -a3 [https://www.facebook.com](https://www.facebook.com) -v|Technology identification, including content management systems (CMS), blogging platforms, statistic/analytics packages, JavaScript libraries, web servers, and embedded devices|
|Wappalyzer|[https://www.wappalyzer.com/](https://www.wappalyzer.com/)|
|sudo apt install wafw00f -y<br><br>wafw00f -v [https://$TARGET](https://$TARGET)|WAF Fingerprinting.|
|Aquatone<br><br>anasec@htb[/htb]$ sudo apt install golang chromium-driver<br><br>anasec@htb[/htb]$ go get github.com/michenriksen/aquatone<br><br>anasec@htb[/htb]$ export PATH="$PATH":"$HOME/go/bin"<br><br>Now, it's time to use cat in our subdomain list and pipe the command to aquatone via:<br><br>cat facebook_aquatone.txt \| aquatone -out ./aquatone -screenshot-timeout 1000|[https://github.com/michenriksen/aquatone](https://github.com/michenriksen/aquatone)<br><br>automatic and visual inspection of websites across many hosts and is convenient for quickly gaining an overview of HTTP-based attack surfaces by scanning a list of configurable ports, visiting the website with a headless Chrome browser, and taking and screenshot.|
|cat subdomain.list \| aquatone -out ./aquatone -screenshot-timeout 1000|Makes screenshots of all subdomains in the subdomain.list.|

  
 

## Active Subdomain Enumeration

|   |   |
|---|---|
|Resource/Command|Description|
|[https://hackertarget.com/zone-transfer/](https://hackertarget.com/zone-transfer/)|HackerTarget - Online Test of a zone transfer that will attempt to get all DNS records for a target domain. The zone transfer will be tested against all name servers (NS) for a domain.|
|[https://github.com/danielmiessler/SecLists](https://github.com/danielmiessler/SecLists)|SecLists|
|nslookup -type=any -query=AXFR $TARGET nameserver.target.domain|Zone Transfer using Nslookup against the target domain and its nameserver.|
|Perform the Zone transfer using -type=any and -query=AXFR parameters:<br><br>nslookup -type=any -query=AXFR exampledomain.com nsztm1.digi.ninja|Identifying Nameservers|
|gobuster dns -q -r "${NS}" -d "${TARGET}" -w "${WORDLIST}" -p ./patterns.txt -o "gobuster_${TARGET}.txt"<br><br>The first step will be to create a patterns.txt file with the patterns previously discovered, for example:<br><br>lert-api-shv-{NUMBER}-sin6.facebook.com<br><br>lert-api-shv-{GOBUSTER}-sin6<br><br>atlas-pp-shv-{GOBUSTER}-sin6<br><br>The next step will be to launch gobuster using the dns module, specifying the following options:<br><br>dns: Launch the DNS module<br><br>-q: Don't print the banner and other noise.<br><br>-r: Use custom DNS server<br><br>-d: A target domain name<br><br>-p: Path to the patterns file<br><br>-w: Path to the wordlist<br><br>-o: Output file<br><br>anasec@htb[/htb]$ export TARGET="facebook.com"<br><br>anasec@htb[/htb]$ export NS="d.ns.facebook.com"<br><br>anasec@htb[/htb]$ export WORDLIST="numbers.txt"<br><br>anasec@htb[/htb]$ gobuster dns -q -r "${NS}" -d "${TARGET}" -w "${WORDLIST}" -p ./patterns.txt -o "gobuster_${TARGET}.txt"|Bruteforcing subdomains with Gobuster|

dig @10.129.238.218 NS axfr internal.inlanefreight.htb (I NEED TO FIGURE OUT WHY THIS COMMAND WORKED to give me the answer. I think it executed a zone transfer on a second zone I enumerated from first zone transfer, but this particular command is nowhere in my notes.

  
 

## Virtual Hosts

|   |   |
|---|---|
|Resource/Command|Description|
|curl -s [http://192.168.10.10](http://192.168.10.10) -H "Host: randomtarget.com"|Changing the HOST HTTP header to request a specific domain.|
|cat ./vhosts.list \| while read vhost;do echo "\n********\nFUZZING: ${vhost}\n********";curl -s -I [http://<IP](http://%3cIP) address> -H "HOST: ${vhost}.target.domain" \| grep "Content-Length: ";done|Bruteforcing for possible virtual hosts on the target domain.|
|ffuf -w ./vhosts -u [http://<IP](http://%3cIP) address> -H "HOST: FUZZ.target.domain" -fs 612|Bruteforcing for possible virtual hosts on the target domain using ffuf.|

  
 

## Crawling

|   |   |
|---|---|
|Resource/Command|Description|
|ZAP|[https://www.zaproxy.org/](https://www.zaproxy.org/)|
|ffuf -recursion -recursion-depth 1 -u [http://192.168.10.10/FUZZ](http://192.168.10.10/FUZZ) -w /opt/useful/SecLists/Discovery/Web-Content/raft-small-directories-lowercase.txt|Discovering files and folders that cannot be spotted by browsing the website using ffuf|
|The first step will be to create a file with the following folder names and save it as folders.txt:<br><br>wp-admin<br><br>wp-content<br><br>wp-includes<br><br>Next, we will extract some keywords from the website using CeWL. We will instruct the tool to extract words with a minimum length of 5 characters -m5, convert them to lowercase --lowercase and save them into a file called wordlist.txt -w <FILE>:<br><br>anasec@htb[/htb]$ cewl -m5 --lowercase -w wordlist.txt [http://192.168.10.10](http://192.168.10.10)<br><br>The next step will be to combine everything in ffuf to see if we can find some juicy information. For this, we will use the following parameters in ffuf:<br><br>-w: We separate the wordlists by coma and add an alias to them to inject them as fuzzing points later<br><br>-u: Our target URL with the fuzzing points.<br><br>ffuf -w ./folders.txt:FOLDERS,./wordlist.txt:WORDLIST,./extensions.txt:EXTENSIONS -u [http://www.target.domain/FOLDERS/WORDLISTEXTENSIONS](http://www.target.domain/FOLDERS/WORDLISTEXTENSIONS)<br><br>Fuff returns us results of:<br><br>[Status: 200, Size: 8, Words: 1, Lines: 2]<br><br>    * EXTENSIONS: ~<br><br>    * FOLDERS: wp-content<br><br>    * WORDLIST: secret<br><br>anasec@htb[/htb]$ curl [http://192.168.10.10/wp-content/secret~](http://192.168.10.10/wp-content/secret~)|Mutated bruteforcing against the target web server using ffuf to find sensitive information.|